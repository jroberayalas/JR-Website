---
title: Notes on Statistical Rethinking (Chapter 6 - Overfitting, Regularization, and
  Information Criteria)
author: José Roberto Ayala Solares
date: '2018-04-06'
slug: notes-on-statistical-rethinking-chapter-6-overfitting-regularization-and-information-criteria
categories:
  - StatisticalRethinking
tags:
  - bayesian
  - notes
summary: Notes for Chapter 6 of [Statistical Rethinking](http://xcelab.net/rm/statistical-rethinking/)
---



<blockquote>
<p>Ockham’s razor: Models with fewer assumptions are to be preferred.</p>
</blockquote>
<blockquote>
<p><strong>Stargazing</strong>. The most common form of model selection among practicing scientists is to search for a model in which every coefficient is statistically significant. Statisticians sometimes call this stargazing, as it is embodied by scanning for asterisks (**) trailing after estimates. The model that is full of stars, the thinking goes, is best. But such a model is not best. Whatever you think about null hypothesis significance testing in general, using it to select among structurally different models is a mistake. p-values are not designed to help you navigate between underfitting and overfitting. It is true that predictor variables that do improve prediction are not always statistically significant. It is also possible for variables that are statistically significant to do nothing useful for prediction. Since the conventional 5% threshold is purely conventional, we shouldn’t expect it to optimize anything.</p>
</blockquote>
<div id="the-problem-with-parameters" class="section level2">
<h2>6.1. The problem with parameters</h2>
<blockquote>
<p>Adding parameters (i.e. making the model more complex) nearly always improves the fit of a model to the data. However, while more complex models fit the data better, they often predict new data worse. But simple models, with too few parameters, tend instead to underfit, systematically overpredicting or underpredicting the data, regardless of how well future data resemble past data. So we can’t always favor either simple models or complex models.</p>
</blockquote>
<div id="more-parameters-always-improve-fit" class="section level3">
<h3>6.1.1. More parameters always improve fit</h3>
<blockquote>
<p>If you adopt a model family with enough parameters, you can fit the data exactly. But such a model will make rather absurd predictions for yet-to-be-observed cases.</p>
</blockquote>
<blockquote>
<p>Model fitting can be considered a form of data compression. Parameters summarize relationships among the data. These summaries compress the data into a simpler form, although with loss of information (“lossy” compression) about the sample. The parameters can then be used to generate new data, effectively decompressing the data. However, when a model has a parameter to correspond to each datum, then there is actually no compression. The model just encodes the raw data in a different form, using parameters instead. As a result, we learn nothing about the data from such a model. Learning about the data requires using a simpler model that achieves some compression, but not too much. This view of model selection is often known as <strong>Minimum Description Length (MDL)</strong>.</p>
</blockquote>
<p>{{% alert note %}} Overfitting produces models that fit the data extremely well, but they suffer for this within-sample accuracy by making nonsensical out-of sample predictions. An overfit model is very sensitive to the sample. {{% /alert %}}</p>
</div>
<div id="too-few-parameters-hurts-too" class="section level3">
<h3>6.1.2. Too few parameters hurts, too</h3>
<p>{{% alert note %}} Underfitting produces models that are inaccurate both within and out of sample. An underfit model is insensitive to the sample. {{% /alert %}}</p>
<p>{{% alert note %}} The underfitting/overfitting dichotomy is often described as the <strong>bias-variance trade-off</strong>. “Bias” is related to underfitting, while “variance” is related to overfitting. {{% /alert %}}</p>
</div>
</div>
<div id="information-theory-and-model-performance" class="section level2">
<h2>6.2. Information theory and model performance</h2>
<blockquote>
<p>In defining a target, there are two major dimensions to worry about:</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li><em>Cost-benefit analysis</em>. How much does it cost when we’re wrong? How much do we win when we’re right?</li>
<li><em>Accuracy in context</em>. Some prediction tasks are inherently easier than others. So even if we ignore costs and benefits, we still need a way to judge “accuracy” that accounts for how much a model could possibly improve prediction.</li>
</ol>
</blockquote>
<blockquote>
<p><strong>What is a true model?</strong> It’s hard to define “true” probabilities, because all models are false. So what does “truth” mean in this context? It means the right probabilities, given our state of ignorance. Our state of ignorance is described by the model. <strong>The probability is in the model, not in the world.</strong> If we had all of the information relevant to producing a forecast, then rain or sun would be deterministic, and the “true” probabilities would be just 0’s and 1’s. Absent some relevant information, as in all modeling, outcomes in the small world are uncertain, even though they remain perfectly deterministic in the large world. Because of our ignorance, we can have “true” probabilities between zero and one.</p>
</blockquote>
<div id="information-and-uncertainty" class="section level3">
<h3>6.2.2. Information and uncertainty</h3>
<blockquote>
<p><em>Information</em>: The reduction in uncertainty derived from learning an outcome.</p>
</blockquote>
<blockquote>
<p><strong>Information entropy</strong>: the uncertainty contained in a probability distribution is the average log probability of an event.</p>
</blockquote>
<p><span class="math display">\[H(p) = - \text{E}\ln(p_i) = -\sum _{ i=1 }^{ n }p_i\ln(p_i)\]</span></p>
<blockquote>
<p>L’Hôpital’s rule tells us that <span class="math inline">\(\lim _{ { p }_{ i }\rightarrow 0 }p_i\ln(p_i) = 0\)</span>.</p>
</blockquote>
</div>
<div id="from-entropy-to-accuracy" class="section level3">
<h3>6.2.3. From entropy to accuracy</h3>
<blockquote>
<p><strong>(Kullback-Leibler or KL) Divergence</strong>: The additional uncertainty induced by using probabilities from one distribution to describe another distribution, i.e. the average difference in log probability between the target (p) and model (q). Divergence is measuring how far q is from the target p, in units of entropy.</p>
</blockquote>
<p><span class="math display">\[D_{KL}(p,q) = \sum _{ i }p_i\left( \ln(p_i)-\ln(q_i) \right) = \sum _{ i }p_i \ln\left(\frac { p_i }{ q_i }\right) \]</span></p>
<blockquote>
<p>Divergence depends upon direction, i.e. <span class="math inline">\(D_{KL}(p,q) \neq D_{KL}(q,p)\)</span>.</p>
</blockquote>
</div>
<div id="from-divergence-to-deviance" class="section level3">
<h3>6.2.4. From divergence to deviance</h3>
<blockquote>
<p>To use <span class="math inline">\(D_{KL}\)</span> to compare models, it seems like we would have to know <span class="math inline">\(p\)</span>, the target probability distribution. But there’s an amazing way out of this predicament. It helps that we are only interested in comparing the divergences of different candidates, say <span class="math inline">\(q\)</span> and <span class="math inline">\(r\)</span>. All of this also means that all we need to know is a model’s average log-probability: <span class="math inline">\(\text{E}\ln(q_i)\)</span> for <span class="math inline">\(q\)</span> and <span class="math inline">\(\text{E}\ln(r_i)\)</span> for <span class="math inline">\(r\)</span>. Neither <span class="math inline">\(\text{E}\ln(q_i)\)</span> nor <span class="math inline">\(\text{E}\ln(r_i)\)</span> by itself suggests a good or bad model. Only the difference <span class="math inline">\(\text{E}\ln(q_i)-\text{E}\ln(r_i)\)</span> informs us about the divergence of each model from the target <span class="math inline">\(p\)</span>.</p>
</blockquote>
<blockquote>
<p>To approximate the relative value of <span class="math inline">\(\text{E}\ln(q_i)\)</span>, we can use a model’s deviance, which is defined as:</p>
</blockquote>
<p><span class="math display">\[D(q) = -2\sum _{ i } \ln\left(q_i\right)\]</span></p>
<blockquote>
<p>where <span class="math inline">\(q_i\)</span> is just the likelihood of observation <span class="math inline">\(i\)</span>.</p>
</blockquote>
<pre class="r"><code>library(tidyverse)

d &lt;- 
    tibble(species = c(&quot;afarensis&quot;, &quot;africanus&quot;, &quot;habilis&quot;, &quot;boisei&quot;,
                       &quot;rudolfensis&quot;, &quot;ergaster&quot;, &quot;sapiens&quot;), 
           brain = c(438, 452, 612, 521, 752, 871, 1350), 
           mass = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5))

d &lt;-
    d %&gt;%
    mutate(mass.s = (mass - mean(mass))/sd(mass))

library(brms)</code></pre>
<pre><code>## Warning: package &#39;Rcpp&#39; was built under R version 3.4.4</code></pre>
<pre class="r"><code># Here we specify our starting values
Inits &lt;- list(Intercept = mean(d$brain),
              mass.s = 0,
              sigma = sd(d$brain))

# List of lists depending on the number of chains
InitsList &lt;-list(Inits, Inits, Inits, Inits)

# The model
m6.8 &lt;- 
  brm(data = d, family = gaussian,
      brain ~ 1 + mass.s,
      prior = c(set_prior(&quot;normal(0, 1000)&quot;, class = &quot;Intercept&quot;),
                set_prior(&quot;normal(0, 1000)&quot;, class = &quot;b&quot;),
                set_prior(&quot;cauchy(0, 10)&quot;, class = &quot;sigma&quot;)),
      chains = 4, iter = 2000, warmup = 1000, cores = 4,
      inits = InitsList)  # Here we put our start values in the brm() function

dfLL &lt;-
    m6.8 %&gt;%
    #log_lik() returns a matrix. Each observation gets a column and each HMC chain iteration gets a row
    log_lik() %&gt;% 
    as_tibble()

dfLL %&gt;%
    glimpse()</code></pre>
<pre><code>## Observations: 4,000
## Variables: 7
## $ V1 &lt;dbl&gt; -6.353431, -6.696489, -6.852660, -7.106943, -6.874477, -6.3...
## $ V2 &lt;dbl&gt; -6.319304, -6.605869, -6.830819, -7.079085, -6.887005, -6.2...
## $ V3 &lt;dbl&gt; -6.582701, -6.461469, -6.826911, -6.851801, -7.081520, -6.5...
## $ V4 &lt;dbl&gt; -6.356792, -6.655955, -6.821925, -6.952974, -6.903588, -6.4...
## $ V5 &lt;dbl&gt; -6.415585, -6.607567, -6.804482, -6.815707, -7.056214, -7.4...
## $ V6 &lt;dbl&gt; -6.388764, -6.537840, -6.838850, -6.912083, -7.205562, -7.8...
## $ V7 &lt;dbl&gt; -9.280646, -8.305840, -8.424597, -8.377968, -9.350525, -7.5...</code></pre>
<pre class="r"><code>dfLL &lt;-
    dfLL %&gt;%
    mutate(sums     = rowSums(.),
           deviance = -2*sums)</code></pre>
<pre class="r"><code>quantile(dfLL$deviance, c(.025, .5, .975))</code></pre>
<pre><code>##      2.5%       50%     97.5% 
##  95.16831  97.49059 105.35478</code></pre>
<p>{{% alert note %}} Since we have a posterior distribution of parameter values, there is also a posterior distribution of the deviance. {{% /alert %}}</p>
<pre class="r"><code>ggplot(dfLL, aes(x = deviance)) +
  theme_classic() +
  geom_density(fill = &quot;plum&quot;, size = 0) +
  geom_vline(xintercept = quantile(dfLL$deviance, c(.025, .5, .975)),
             color = &quot;plum4&quot;, linetype = c(2, 1, 2)) +
  scale_x_continuous(breaks = quantile(dfLL$deviance, c(.025, .5, .975)),
                     labels = c(95, 98, 105)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = &quot;The deviance distribution&quot;,
       subtitle = &quot;The dotted lines are the 95% intervals and\nthe solid line is the median.&quot;) +
  theme(text = element_text(family = &quot;Courier&quot;))</code></pre>
<p><img src="/post/2018-04-06-notes-on-statistical-rethinking-chapter-6-overfitting-regularization-and-information-criteria_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="from-deviance-to-out-of-sample" class="section level3">
<h3>6.2.5. From deviance to out-of-sample</h3>
<p>{{% alert note %}} <strong>Deviance is an assessment of predictive accuracy, not of truth</strong>. The true model, in terms of which predictors are included, is not guaranteed to produce the best predictions. Likewise a false model, in terms of which predictors are included, is not guaranteed to produce poor predictions. {{% /alert %}}</p>
<p>{{% alert note %}} While deviance on training data always improves with additional predictor variables, deviance on future data may or may not, depending upon both the true data-generating process and how much data is available to precisely estimate the parameters. These facts form the basis for understanding both regularizing priors and information criteria. {{% /alert %}}</p>
</div>
</div>
<div id="regularization" class="section level2">
<h2>6.3. Regularization</h2>
<blockquote>
<p>When the priors are flat or nearly flat, the machine interprets this to mean that every parameter value is equally plausible. As a result, the model returns a posterior that encodes as much of the training sample (as represented by the likelihood function) as possible.</p>
</blockquote>
<blockquote>
<p>A regularizing prior, when tuned properly, reduces overfitting while still allowing the model to learn the regular features of a sample. If the prior is too skeptical, however, then regular features will be missed, resulting in underfitting. So the problem is really one of tuning. Even mild skepticism can help a model do better, and doing better is all we can really hope for in the large world, where <strong>no model nor prior is optimal</strong>.</p>
</blockquote>
<blockquote>
<p>Regularizing priors are great, because they reduce overfitting. But if they are too skeptical, they prevent the model from learning from the data. So to use them most effectively, you need some way to tune them. Tuning them isn’t always easy. If you have enough data, you can split into “train” and “test” samples and then try different priors and select the one that provides the smallest deviance on the test sample. That is the essence of <strong>cross-validation</strong>, a common technique for reducing overfitting. But if you need to use all of the data to train the model, tuning the prior may not be so easy. It would be nice to have a way to predict a model’s out-of-sample deviance, to forecast its predictive accuracy, using only the sample at hand. That’s the purpose of <strong>information criteria</strong>.</p>
</blockquote>
<p>{{% alert note %}} Linear models in which the slope parameters use Gaussian priors, centered at zero, are sometimes known as <strong>ridge regression</strong>. Ridge regression typically takes as input a precision <span class="math inline">\(\lambda\)</span> that essentially describes the narrowness of the prior. <span class="math inline">\(\lambda &gt; 0\)</span> results in less overfitting. However, just as with the Bayesian version, if <span class="math inline">\(\lambda\)</span> is too large, we risk underfitting. {{% /alert %}}</p>
</div>
<div id="information-criteria" class="section level2">
<h2>6.4. Information criteria</h2>
<p>The phenomenon behind information criteria is that it is possible to trace the average out-of-sample deviance for each model.</p>
<p>{{% alert warning %}} Information criteria are not consistent for model identification. These criteria aim to nominate the model that will produce the best predictions, as judged by out-of-sample deviance. {{% /alert %}}</p>
<div id="deviance-information-criterion-dic" class="section level3">
<h3>6.4.1. Deviance Information Criterion (DIC)</h3>
<blockquote>
<p>DIC is essentially a version of AIC that is aware of informative priors. Like AIC, it assumes a multivariate Gaussian posterior distribution. DIC is calculated from the posterior distribution of the training deviance.</p>
</blockquote>
<p>{{% alert warning %}} If any parameter in the posterior is substantially skewed, and also has a substantial effect on prediction, then DIC like AIC can go horribly wrong. {{% /alert %}}</p>
<blockquote>
<p>DIC is calculated as:</p>
</blockquote>
<p><span class="math display">\[\text{DIC}=\bar { D } +\left( \bar { D } -\hat { D }  \right) =\bar { D } -{ p }_{ D }\]</span></p>
<blockquote>
<p>where <span class="math inline">\(D\)</span> is the posterior distribution of deviance, <span class="math inline">\(\bar{D}\)</span> indicates the average of <span class="math inline">\(D\)</span>, and <span class="math inline">\(\hat{D}\)</span> is the deviance calculated at the posterior mean. The difference <span class="math inline">\(\bar { D } -\hat { D } = { p }_{ D }\)</span> is analogous to the number of parameters used in computing AIC. It is an “effective” number of parameters that measures how flexible the model is in fitting the training sample.</p>
</blockquote>
<blockquote>
<p>The <span class="math inline">\(p_D\)</span> term is sometimes called a penalty term. It is just the expected distance between the deviance in-sample and the deviance out-of-sample. In the case of flat priors, DIC reduces directly to AIC, because the expected distance is just the number of parameters. But more generally, <span class="math inline">\(p_D\)</span> will be some fraction of the number of parameters, because regularizing priors constrain a model’s flexibility.</p>
</blockquote>
</div>
<div id="widely-applicable-information-criterion-waic" class="section level3">
<h3>6.4.2 Widely Applicable Information Criterion (WAIC)</h3>
<blockquote>
<p>WAIC is just an estimate of out-of-sample deviance. It does not require a multivariate Gaussian posterior, and it is often more accurate than DIC. The distinguishing feature of WAIC is that it is pointwise. This means that uncertainty in prediction is considered case-by-case, or point-by-point, in the data.</p>
</blockquote>
<blockquote>
<p>Define <span class="math inline">\(\Pr(y_i)\)</span> as the average likelihood of observation <span class="math inline">\(i\)</span> in the training sample. This means we compute the likelihood of <span class="math inline">\(y_i\)</span> for each set of parameters sampled from the posterior distribution. Then we average the likelihoods for each observation <span class="math inline">\(i\)</span> and finally sum over all observations. This produces the first part of WAIC, the log-pointwise-predictive-density, lppd:</p>
</blockquote>
<p><span class="math display">\[\text{lppd} = \sum _{ i } ^{N} \ln \Pr\left(y_i\right)\]</span></p>
<blockquote>
<p>The lppd is just a pointwise analog of deviance, averaged over the posterior distribution. If you multiplied it by -2, it’d be similar to the deviance.</p>
</blockquote>
<blockquote>
<p>Define <span class="math inline">\(V(y_i)\)</span> as the variance in log-likelihood for observation <span class="math inline">\(i\)</span> in the training sample. This means we compute the log-likelihood of <span class="math inline">\(y_i\)</span> for each sample from the posterior distribution. Then we take the variance of those values. Then the effective number of parameters <span class="math inline">\(p_{WAIC}\)</span> is defined as:</p>
</blockquote>
<p><span class="math display">\[p_{WAIC} = \sum _{ i } ^{N} V\left(y_i\right)\]</span></p>
<blockquote>
<p>WAIC is defined as:</p>
</blockquote>
<p><span class="math display">\[\text{WAIC} = -2\left( \text{lppd} - p_{WAIC} \right)\]</span></p>
<pre class="r"><code>waic(m6.8)</code></pre>
<pre><code>##   WAIC  SE
##  102.2 6.5</code></pre>
<p>{{% alert warning %}} Because WAIC requires splitting up the data into independent observations, it is sometimes hard to define. In time series, you can compute WAIC as if each observation were independent of the others, but it’s not clear what the resulting value means. {{% /alert %}}</p>
<p>{{% alert warning %}} A more general issue with all of these predictive information criteria: their validity depends upon the predictive task at hand. {{% /alert %}}</p>
</div>
</div>
<div id="using-information-criteria" class="section level2">
<h2>6.5. Using information criteria</h2>
<blockquote>
<p>Frequently, people discuss model selection, which usually means choosing the model with the lowest AIC/DIC/WAIC value and then discarding the others. But this kind of selection procedure discards the information about relative model accuracy contained in the differences among the AIC/DIC/WAIC values. This information is useful because sometimes the differences are large and sometimes they are small. Just as relative posterior probability provides advice about how confident we might be about parameters (conditional on the model), relative model accuracy provides advice about how confident we might be about models (conditional on the set of models compared).</p>
</blockquote>
<div id="model-comparison" class="section level3">
<h3>6.5.1. Model comparison</h3>
<blockquote>
<p>Model comparison means using DIC/WAIC in combination with the estimates and posterior predictive checks from each model. It is just as important to understand why a model outperforms another as it is to measure the performance difference.</p>
</blockquote>
<p>{{% alert warning %}} Compared models must be fit to exactly the same observations. A model fit to fewer observations will almost always have a better deviance and AIC/DIC/WAIC value, because it has been asked to predict less. {{% /alert %}}</p>
<pre class="r"><code>library(rethinking)

data(milk)
d &lt;- 
  milk %&gt;%
  filter(complete.cases(.))
rm(milk)

d &lt;-
  d %&gt;%
  mutate(neocortex = neocortex.perc/100)

detach(package:rethinking, unload = T)

# Model with no predictors (just the intercept)
Inits &lt;- list(Intercept = mean(d$kcal.per.g),
              sigma = sd(d$kcal.per.g))
InitsList &lt;-list(Inits, Inits, Inits, Inits)
m6.11 &lt;- 
    brm(data = d, family = gaussian,
        kcal.per.g ~ 1,
        prior = c(set_prior(&quot;uniform(-1000, 1000)&quot;, class = &quot;Intercept&quot;),
                  set_prior(&quot;uniform(0, 100)&quot;, class = &quot;sigma&quot;)),
        chains = 4, iter = 2000, warmup = 1000, cores = 4,
        inits = InitsList)</code></pre>
<pre><code>## Warning: It appears as if you have specified an upper bounded prior on a parameter that has no natural upper bound.
## If this is really what you want, please specify argument &#39;ub&#39; of &#39;set_prior&#39; appropriately.
## Warning occurred for prior 
## sigma ~ uniform(0, 100)</code></pre>
<pre class="r"><code># Model with only neocortex
Inits &lt;- list(Intercept = mean(d$kcal.per.g),
              neocortex = 0,
              sigma = sd(d$kcal.per.g))
InitsList &lt;-list(Inits, Inits, Inits, Inits)
m6.12 &lt;- 
    brm(data = d, family = gaussian,
        kcal.per.g ~ 1 + neocortex,
        prior = c(set_prior(&quot;uniform(-1000, 1000)&quot;, class = &quot;Intercept&quot;),
                  set_prior(&quot;uniform(-1000, 1000)&quot;, class = &quot;b&quot;),
                  set_prior(&quot;uniform(0, 100)&quot;, class = &quot;sigma&quot;)),
        chains = 4, iter = 2000, warmup = 1000, cores = 4,
        inits = InitsList)</code></pre>
<pre><code>## Warning: It appears as if you have specified a lower bounded prior on a parameter that has no natural lower bound.
## If this is really what you want, please specify argument &#39;lb&#39; of &#39;set_prior&#39; appropriately.
## Warning occurred for prior 
## b ~ uniform(-1000, 1000)</code></pre>
<pre><code>## Warning: It appears as if you have specified an upper bounded prior on a parameter that has no natural upper bound.
## If this is really what you want, please specify argument &#39;ub&#39; of &#39;set_prior&#39; appropriately.
## Warning occurred for prior 
## b ~ uniform(-1000, 1000)
## sigma ~ uniform(0, 100)</code></pre>
<pre class="r"><code># Model with only log mass
Inits &lt;- list(Intercept = mean(d$kcal.per.g),
              `log(mass)` = 0,
              sigma = sd(d$kcal.per.g))
InitsList &lt;-list(Inits, Inits, Inits, Inits)
m6.13 &lt;- brm(data = d, family = gaussian,
             kcal.per.g ~ 1 + log(mass),
             prior = c(set_prior(&quot;uniform(-1000, 1000)&quot;, class = &quot;Intercept&quot;),
                       set_prior(&quot;uniform(-1000, 1000)&quot;, class = &quot;b&quot;),
                       set_prior(&quot;uniform(0, 100)&quot;, class = &quot;sigma&quot;)),
             chains = 4, iter = 2000, warmup = 1000, cores = 4,
             inits = InitsList)</code></pre>
<pre><code>## Warning: It appears as if you have specified a lower bounded prior on a parameter that has no natural lower bound.
## If this is really what you want, please specify argument &#39;lb&#39; of &#39;set_prior&#39; appropriately.
## Warning occurred for prior 
## b ~ uniform(-1000, 1000)

## Warning: It appears as if you have specified an upper bounded prior on a parameter that has no natural upper bound.
## If this is really what you want, please specify argument &#39;ub&#39; of &#39;set_prior&#39; appropriately.
## Warning occurred for prior 
## b ~ uniform(-1000, 1000)
## sigma ~ uniform(0, 100)</code></pre>
<pre class="r"><code># Model with both neocortex and log mass
Inits &lt;- list(Intercept = mean(d$kcal.per.g),
              neocortex = 0,
              `log(mass)` = 0,
              sigma = sd(d$kcal.per.g))
InitsList &lt;-list(Inits, Inits, Inits, Inits)
m6.14 &lt;- 
    brm(data = d, family = gaussian,
        kcal.per.g ~ 1 + neocortex + log(mass),
        prior = c(set_prior(&quot;uniform(-1000, 1000)&quot;, class = &quot;Intercept&quot;),
                  set_prior(&quot;uniform(-1000, 1000)&quot;, class = &quot;b&quot;),
                  set_prior(&quot;uniform(0, 100)&quot;, class = &quot;sigma&quot;)),
        chains = 4, iter = 2000, warmup = 1000, cores = 4,
        inits = InitsList)</code></pre>
<pre><code>## Warning: It appears as if you have specified a lower bounded prior on a parameter that has no natural lower bound.
## If this is really what you want, please specify argument &#39;lb&#39; of &#39;set_prior&#39; appropriately.
## Warning occurred for prior 
## b ~ uniform(-1000, 1000)

## Warning: It appears as if you have specified an upper bounded prior on a parameter that has no natural upper bound.
## If this is really what you want, please specify argument &#39;ub&#39; of &#39;set_prior&#39; appropriately.
## Warning occurred for prior 
## b ~ uniform(-1000, 1000)
## sigma ~ uniform(0, 100)</code></pre>
<div id="comparing-waic-values" class="section level4">
<h4>6.5.1.1. Comparing WAIC values</h4>
<p>One way to compare models is using the <code>waic</code> function:</p>
<pre class="r"><code>waic(m6.11, m6.12, m6.13, m6.14)</code></pre>
<pre><code>##                 WAIC   SE
## m6.11          -8.80 3.74
## m6.12          -7.33 3.22
## m6.13          -8.79 4.14
## m6.14         -17.07 5.08
## m6.11 - m6.12  -1.46 1.22
## m6.11 - m6.13  -0.01 2.28
## m6.11 - m6.14   8.27 4.85
## m6.12 - m6.13   1.45 2.96
## m6.12 - m6.14   9.73 4.96
## m6.13 - m6.14   8.28 3.44</code></pre>
<p>Alternatively, first compute the WAIC for each model and use the <code>compare_ic</code> function:</p>
<pre class="r"><code>w.m6.11 &lt;- waic(m6.11)
w.m6.12 &lt;- waic(m6.12)
w.m6.13 &lt;- waic(m6.13)
w.m6.14 &lt;- waic(m6.14)

compare_ic(w.m6.11, w.m6.12, w.m6.13, w.m6.14)</code></pre>
<pre><code>##                 WAIC   SE
## m6.11          -8.80 3.74
## m6.12          -7.33 3.22
## m6.13          -8.79 4.14
## m6.14         -17.07 5.08
## m6.11 - m6.12  -1.46 1.22
## m6.11 - m6.13  -0.01 2.28
## m6.11 - m6.14   8.27 4.85
## m6.12 - m6.13   1.45 2.96
## m6.12 - m6.14   9.73 4.96
## m6.13 - m6.14   8.28 3.44</code></pre>
<pre class="r"><code>tibble(model = c(&quot;m6.11&quot;, &quot;m6.12&quot;, &quot;m6.13&quot;, &quot;m6.14&quot;),
       waic = c(w.m6.11$waic, w.m6.12$waic, w.m6.13$waic, w.m6.14$waic),
       se = c(w.m6.11$se_waic, w.m6.12$se_waic, w.m6.13$se_waic, w.m6.14$se_waic)) %&gt;%
  
  ggplot() +
  theme_classic() +
  geom_pointrange(aes(x = model, y = waic, 
                      ymin = waic - se, 
                      ymax = waic + se),
                  shape = 21, color = &quot;plum4&quot;, fill = &quot;plum&quot;) +
  coord_flip() +
  labs(x = NULL, y = NULL,
       title = &quot;My custom WAIC plot&quot;) +
  theme(text = element_text(family = &quot;Courier&quot;),
        axis.ticks.y = element_blank())</code></pre>
<p><img src="/post/2018-04-06-notes-on-statistical-rethinking-chapter-6-overfitting-regularization-and-information-criteria_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>{{% alert note %}} Worth checking the <a href="https://cran.r-project.org/web/packages/loo/vignettes/loo-example.html">LOO information criteria</a>. {{% /alert %}}</p>
<p>I replicated the <code>compare</code> function from the <code>rethinking</code> package. It returns a table in which models are ranked from best to worst, with six columns of information:</p>
<ol style="list-style-type: decimal">
<li>WAIC is the WAIC for each model. Smaller WAIC indicates better estimated out-of-sample deviance.</li>
<li>pWAIC is the estimated effective number of parameters. This provides a clue as to how flexible each model is in fitting the sample.</li>
<li>dWAIC is the difference between each WAIC and the lowest WAIC. Since only relative deviance matters, this column shows the differences in relative fashion.</li>
<li>weight is the Akaike weight for each model. These values are transformed information criterion values.</li>
<li>SE is the standard error of the WAIC estimate. WAIC is an estimate, and provided the sample size N is large enough, its uncertainty will be well approximated by its standard error. So this SE value isn’t necessarily very precise, but it does provide a check against overconfidence in differences between WAIC values.</li>
<li>dSE is the standard error of the difference in WAIC between each model and the top-ranked model. So it is missing for the top model.</li>
</ol>
<pre class="r"><code>compare_waic &lt;- function (..., sort = &quot;WAIC&quot;, func = &quot;WAIC&quot;) 
{
    mnames &lt;- as.list(substitute(list(...)))[-1L]
    L &lt;- list(...)
    if (is.list(L[[1]]) &amp;&amp; length(L) == 1) {L &lt;- L[[1]]}
    #mnames &lt;- match.call()
    #mnames &lt;- as.character(mnames)[2:(length(L) + 1)]
    #the_func &lt;- deparse(substitute(func))
    classes &lt;- as.character(sapply(L, class))
    if (any(classes != classes[1])) {
        warning(&quot;Not all model fits of same class.\nThis is usually a bad idea, because it implies they were fit by different algorithms.\nCheck yourself, before you wreck yourself.&quot;)
    }
    nobs_list &lt;- try(sapply(L, nobs))
    if (any(nobs_list != nobs_list[1])) {
        nobs_out &lt;- paste(mnames, nobs_list, &quot;\n&quot;)
        nobs_out &lt;- concat(nobs_out)
        warning(concat(&quot;Different numbers of observations found for at least two models.\nInformation criteria only valid for comparing models fit to exactly same observations.\nNumber of observations for each model:\n&quot;, 
                       nobs_out))
    }
    dSE.matrix &lt;- matrix(NA, nrow = length(L), ncol = length(L))
    
    if (func == &quot;WAIC&quot;) {
        WAIC.list &lt;- lapply(L, function(z) WAIC(z, pointwise = TRUE))
        p.list &lt;- sapply(WAIC.list, function(x) x$p_waic)
        se.list &lt;- sapply(WAIC.list, function(x) x$se_waic)
        IC.list &lt;- sapply(WAIC.list, function(x) x$waic)
        #mnames &lt;- sapply(WAIC.list, function(x) x$model_name)
        colnames(dSE.matrix) &lt;- mnames
        rownames(dSE.matrix) &lt;- mnames
        for (i in 1:(length(L) - 1)) {
            for (j in (i + 1):length(L)) {
                waic_ptw1 &lt;- WAIC.list[[i]]$pointwise[ , 3]
                waic_ptw2 &lt;- WAIC.list[[j]]$pointwise[ , 3]
                dSE.matrix[i, j] &lt;- as.numeric(sqrt(length(waic_ptw1) * 
                                                        var(waic_ptw1 - waic_ptw2)))
                dSE.matrix[j, i] &lt;- dSE.matrix[i, j]
            }
        }
    }
    
    #if (!(the_func %in% c(&quot;DIC&quot;, &quot;WAIC&quot;, &quot;LOO&quot;))) {
    #    IC.list &lt;- lapply(L, function(z) func(z))
    #}
    IC.list &lt;- unlist(IC.list)
    dIC &lt;- IC.list - min(IC.list)
    w.IC &lt;- rethinking::ICweights(IC.list)
    if (func == &quot;WAIC&quot;) {
        topm &lt;- which(dIC == 0)
        dSEcol &lt;- dSE.matrix[, topm]
        result &lt;- data.frame(WAIC = IC.list, pWAIC = p.list, 
                             dWAIC = dIC, weight = w.IC, SE = se.list, dSE = dSEcol)
    }
    
    #if (!(the_func %in% c(&quot;DIC&quot;, &quot;WAIC&quot;, &quot;LOO&quot;))) {
    #    result &lt;- data.frame(IC = IC.list, dIC = dIC, weight = w.IC)
    #}
    rownames(result) &lt;- mnames
    if (!is.null(sort)) {
        if (sort != FALSE) {
            if (sort == &quot;WAIC&quot;) 
                sort &lt;- func
            result &lt;- result[order(result[[sort]]), ]
        }
    }
    new(&quot;compareIC&quot;, output = result, dSE = dSE.matrix)
}

compare_waic(m6.11, m6.12, m6.13, m6.14)</code></pre>
<pre><code>##        WAIC pWAIC dWAIC weight   SE  dSE
## m6.14 -17.1   3.0   0.0   0.96 5.08   NA
## m6.11  -8.8   1.3   8.3   0.02 3.74 4.85
## m6.13  -8.8   2.0   8.3   0.02 4.14 3.44
## m6.12  -7.3   1.9   9.7   0.01 3.22 4.96</code></pre>
<p>Akaike weights help by rescaling. The weight for a model <span class="math inline">\(i\)</span> in a set of <span class="math inline">\(m\)</span> models is given by:</p>
<p><span class="math display">\[w_i = \frac {\exp \left( -\frac { 1 }{ 2 } \text{dWAIC}_i \right)} {\sum _{j=1}^{m}{\exp \left( -\frac { 1 }{ 2 } \text{dWAIC}_j \right)} }\]</span> where <span class="math inline">\(\text{dWAIC}_j\)</span> is the same as dWAIC from the compare table output.</p>
<p>{{% alert note %}} A model’s weight is an estimate of the probability that the model will make the best predictions on new data, conditional on the set of models considered. {{% /alert %}}</p>
<blockquote>
<p>WAIC as the expected deviance of a model on future data. That is to say that WAIC gives us an estimate of <span class="math inline">\(\text{E}(D_{\text{test}})\)</span>. Akaike weights convert these deviance values, which are log-likelihoods, to plain likelihoods and then standardize them all.</p>
</blockquote>
<p>{{% alert warning %}} The Akaike weights are analogous to posterior probabilities of models, conditional on expected future data. However, given all the strong assumptions about repeat sampling that go into calculating WAIC, you can’t take this heuristic too seriously. The future is unlikely to be exactly like the past, after all. {{% /alert %}}</p>
</div>
<div id="comparing-estimates" class="section level4">
<h4>6.5.1.2. Comparing estimates</h4>
<blockquote>
<p>Comparing estimates helps in at least two major ways. First, it is useful to understand why a particular model or models have lower WAIC values. Changes in posterior distributions, across models, provide useful hints. Second, regardless of WAIC values, we often want to know whether some parameter’s posterior distribution is stable across models.</p>
</blockquote>
<pre class="r"><code>library(broom)

my_coef_tab &lt;-
  rbind(tidy(m6.11), tidy(m6.12), tidy(m6.13), tidy(m6.14)) %&gt;%
  mutate(model = c(rep(&quot;m6.11&quot;, times = nrow(tidy(m6.11))),
                   rep(&quot;m6.12&quot;, times = nrow(tidy(m6.12))),
                   rep(&quot;m6.13&quot;, times = nrow(tidy(m6.13))),
                   rep(&quot;m6.14&quot;, times = nrow(tidy(m6.14))))
         ) %&gt;%
  filter(term != &quot;lp__&quot;) %&gt;%
  select(model, everything()) %&gt;%
  complete(term = distinct(., term), model) %&gt;%
  rbind(
     tibble(
       model = NA,
       term = c(&quot;b_logmass&quot;, &quot;b_neocortex&quot;, &quot;sigma&quot;, &quot;b_Intercept&quot;),
       estimate = NA,
       std.error = NA,
       lower = NA,
       upper = NA)) %&gt;%
  mutate(axis = ifelse(is.na(model), term, model),
         model = factor(model, levels = c(&quot;m6.11&quot;, &quot;m6.12&quot;, &quot;m6.13&quot;, &quot;m6.14&quot;)),
         term = factor(term, levels = c(&quot;b_logmass&quot;, &quot;b_neocortex&quot;, &quot;sigma&quot;, &quot;b_Intercept&quot;, NA))) %&gt;%
  arrange(term, model) %&gt;%
  mutate(axis_order = letters[1:20],
         axis = ifelse(str_detect(axis, &quot;m6.&quot;), str_c(&quot;      &quot;, axis), axis))
  
ggplot(data = my_coef_tab,
       aes(x = axis_order,
           y = estimate,
           ymin = lower,
           ymax = upper)) +
  theme_classic() +
  geom_hline(yintercept = 0, color = &quot;plum4&quot;, alpha = 1/8) +
  geom_pointrange(shape = 21, color = &quot;plum4&quot;, fill = &quot;plum&quot;) +
  scale_x_discrete(NULL, labels = my_coef_tab$axis) +
  ggtitle(&quot;My coeftab() plot&quot;) +
  coord_flip() +
  theme(panel.grid = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_text(hjust = 0))</code></pre>
<pre><code>## Warning: Removed 8 rows containing missing values (geom_pointrange).</code></pre>
<p><img src="/post/2018-04-06-notes-on-statistical-rethinking-chapter-6-overfitting-regularization-and-information-criteria_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
</div>
<div id="model-averaging" class="section level3">
<h3>6.5.2. Model averaging</h3>
<blockquote>
<p>Model averaging means using DIC/WAIC to construct a posterior predictive distribution that exploits what we know about relative accuracy of the models. This helps guard against overconfidence in model structure, in the same way that using the entire posterior distribution helps guard against overconfidence in parameter values.</p>
</blockquote>
<pre class="r"><code>nd &lt;- 
    tibble(neocortex = seq(from = .5, to = .8, length.out = 30),
           mass = rep(4.5, times = 30))

ftd &lt;-
    fitted(m6.14, newdata = nd) %&gt;%
    as_tibble() %&gt;%
    bind_cols(nd)

pp_average(m6.11, m6.12, m6.13, m6.14,
           weights = &quot;waic&quot;,
           method = &quot;fitted&quot;,  # for new data predictions, use method = &quot;predict&quot;
           newdata = nd) %&gt;%
  as_tibble() %&gt;%
  bind_cols(nd) %&gt;%
  
  ggplot(aes(x = neocortex, y = Estimate)) +
  theme_classic() +
  geom_ribbon(aes(ymin = `2.5%ile`, ymax = `97.5%ile`), 
              fill = &quot;plum&quot;, alpha = 1/3) +
  geom_line(color = &quot;plum2&quot;) +
  geom_ribbon(data = ftd, aes(ymin = `2.5%ile`, ymax = `97.5%ile`),
              fill = &quot;transparent&quot;, color = &quot;plum3&quot;, linetype = 2) +
  geom_line(data = ftd,
              color = &quot;plum3&quot;, linetype = 2) +
  geom_point(data = d, aes(x = neocortex, y = kcal.per.g), 
             size = 2, color = &quot;plum4&quot;) +
  labs(y = &quot;kcal.per.g&quot;) +
  coord_cartesian(xlim = range(d$neocortex), 
                  ylim = range(d$kcal.per.g))</code></pre>
<p><img src="/post/2018-04-06-notes-on-statistical-rethinking-chapter-6-overfitting-regularization-and-information-criteria_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>{{% alert note %}} Model averaging will never make a predictor variable appear more influential than it already appears in any single model. {{% /alert %}}</p>
</div>
</div>
<div id="bonus-r2-revisited" class="section level2">
<h2>Bonus: <span class="math inline">\(R^2\)</span> revisited</h2>
<p>The usual definition of <span class="math inline">\(R^2\)</span> has a problem for Bayesian fits. Check the <a href="https://github.com/jgabry/bayes_R2/blob/master/bayes_R2.pdf">paper</a> by Gelman, Goodrich, Gabry, and Ali.</p>
<pre class="r"><code>bayes_R2(m6.14) %&gt;% round(digits = 3)</code></pre>
<pre><code>##    Estimate Est.Error 2.5%ile 97.5%ile
## R2    0.497     0.128   0.172    0.664</code></pre>
<pre class="r"><code>rbind(bayes_R2(m6.11), 
      bayes_R2(m6.12), 
      bayes_R2(m6.13), 
      bayes_R2(m6.14)) %&gt;%
  as_tibble() %&gt;%
  mutate(model = c(&quot;m6.11&quot;, &quot;m6.12&quot;, &quot;m6.13&quot;, &quot;m6.14&quot;),
         r_square_posterior_mean = round(Estimate, digits = 2)) %&gt;%
  select(model, r_square_posterior_mean)</code></pre>
<pre><code>## # A tibble: 4 x 2
##   model r_square_posterior_mean
##   &lt;chr&gt;                   &lt;dbl&gt;
## 1 m6.11                  0.    
## 2 m6.12                  0.0700
## 3 m6.13                  0.150 
## 4 m6.14                  0.500</code></pre>
<pre class="r"><code># model b6.13
m6.13.R2 &lt;- 
  bayes_R2(m6.13, summary = F) %&gt;%
  as_tibble() %&gt;%
  rename(R2.13 = R2)

# model b6.14
m6.14.R2 &lt;- 
  bayes_R2(m6.14, summary = F) %&gt;%
  as_tibble() %&gt;%
  rename(R2.14 = R2)

# Let&#39;s put them in the same data object
combined_R2s &lt;-
    bind_cols(m6.13.R2, m6.14.R2) %&gt;%
    mutate(dif = R2.14 - R2.13)

# A simple density plot
combined_R2s %&gt;%
  ggplot(aes(x = R2.13)) +
  theme_classic() +
  geom_density(size = 0, fill = &quot;springgreen4&quot;, alpha = 2/3) +
  geom_density(aes(x = R2.14),
               size = 0, fill = &quot;springgreen3&quot;, alpha = 2/3) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = 0:1) +
  labs(x = NULL,
       title = expression(paste(italic(&quot;R&quot;)^{2}, &quot; distributions&quot;)),
       subtitle = &quot;Going from left to right, these are for\nmodels m6.13 and m6.14.&quot;)</code></pre>
<p><img src="/post/2018-04-06-notes-on-statistical-rethinking-chapter-6-overfitting-regularization-and-information-criteria_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code>combined_R2s %&gt;%
  ggplot(aes(x = dif)) +
  theme_classic() +
  geom_density(size = 0, fill = &quot;springgreen&quot;) +
  geom_vline(xintercept = quantile(combined_R2s$dif, 
                                   probs = c(.025, .5, .975)),
             color = &quot;white&quot;, size = c(1/2, 1, 1/2)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(x = expression(paste(Delta, italic(&quot;R&quot;)^{2})),
       subtitle = &quot;This is how much more variance, in terms\nof %, model b6.14 explained compared to\nmodel b6.13. The white lines are the\nposterior median and 95% percentiles.&quot;)</code></pre>
<p><img src="/post/2018-04-06-notes-on-statistical-rethinking-chapter-6-overfitting-regularization-and-information-criteria_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>McElreath, R. (2016). <em>Statistical rethinking: A Bayesian course with examples in R and Stan.</em> Chapman &amp; Hall/CRC Press.</p>
<p>Kurz, A. S. (2018, March 9). <em>brms, ggplot2 and tidyverse code, by chapter</em>. Retrieved from <a href="https://goo.gl/JbvNTj" class="uri">https://goo.gl/JbvNTj</a></p>
</div>
