---
title: Notes on Statistical Rethinking (Chapter 4 - Linear Models)
author: José Roberto Ayala Solares
date: '2018-03-28'
categories:
  - StatisticalRethinking
tags:
  - bayesian
  - notes
slug: notes-on-statistical-rethinking-chapter-4-linear-models
summary: Notes for Chapter 4 of [Statistical Rethinking](http://xcelab.net/rm/statistical-rethinking/)
---

> Linear regression is a family of simple statistical golems that attempt to learn about the mean and variance of some measurement, using an additive combination of other measurements. Linear regression can usefully describe a very large variety of natural phenomena, and it is a descriptive model that corresponds to many different process models.

## 4.1. Why normal distributions are normal
### 4.1.1. Normal by addition
> Any process that adds together random values from the same distribution converges to a normal.

```{r message=FALSE}
library(tidyverse)

set.seed(1000)
pos <- 
  replicate(100, runif(16, -1, 1)) %>%        # Here's the simulation
  as_tibble() %>%                             # For data manipulation, we'll make this a tibble
  rbind(0, .) %>%                             # Here we add a row of zeros above the simulation results
  mutate(step = 0:16) %>%                     # This adds our step index
  gather(key, value, -step) %>%               # Here we convert the data to the long format
  mutate(person = rep(1:100, each = 17)) %>%  # This adds a person id index
  # The next two lines allows us to make culmulative sums within each person
  group_by(person) %>%
  mutate(position = cumsum(value)) %>%
  ungroup()  # Ungrouping allows for further data manipulation

pos %>%
  filter(step == 16) %>%
  ggplot(aes(x = position)) +
  geom_density(color = "dodgerblue1") + #geom_line(stat = "density", color = "dodgerblue1") +
  coord_cartesian(xlim = -6:6) +
  labs(title = "16 steps")
```

### 4.1.2. Normal by multiplication
> Small effects that multiply together are approximately additive, and so they also tend to stabilize on Gaussian distributions.

```{r}
set.seed(.1)
growth <- 
  replicate(10000, prod(1 + runif(12, 0, 0.1))) %>%
  as_tibble()

ggplot(data = growth, aes(x = value)) +
  geom_density(color = "dodgerblue1")
```

### 4.1.3. Normal by log-multiplication
> Large deviates that are multiplied together do not produce Gaussian distributions, but they do tend to produce Gaussian distributions on the log scale.

```{r}
set.seed(12)
replicate(10000, log(prod(1 + runif(12,0,0.5)))) %>%
    as_tibble() %>%
    ggplot(aes(x = value)) +
    geom_density(color = "dodgerblue1")
```

### 4.1.4. Using Gaussian distributions
#### 4.1.4.1. Ontological justification
> The world is full of Gaussian distributions, approximately. As a mathematical idealization, we’re never going to experience a perfect Gaussian distribution. But it is a widespread pattern, appearing again and again at different scales and in different domains. Measurement errors, variations in growth, and the velocities of molecules all tend towards Gaussian distributions. These processes do this because at their heart, these processes add together fluctuations. And repeatedly adding finite fluctuations results in a distribution of sums that have shed all information about the underlying process, aside from mean and spread.
One consequence of this is that statistical models based on Gaussian distributions cannot reliably identify micro-process. But it also means that these models can do useful work, even when they cannot identify process.

#### 4.1.4.2 Epistemological justification
> The Gaussian distribution is the most natural expression of our state of ignorance, because if all we are willing to assume is that a measure has finite variance, the Gaussian distribution is the shape that can be realized in the largest number of ways and does not introduce any new assumptions. **It is the least surprising and least informative assumption to make.**

## 4.3. A Gaussian model of height
### 4.3.1. The data
```{r message=FALSE}
# Load data from the rethinking package
library(rethinking)
data(Howell1)
d <- Howell1

# Have a look at the data
d %>% glimpse()
```

```{r}
# Filter heights of adults in the sample. The reason to filter out non-adults is that height is strongly correlated with age, before adulthood.
d2 <- d %>% filter(age >= 18) %>% glimpse()
```

### 4.3.2. The model
```{r}
d2 %>%
    ggplot(aes(x = height)) + 
    geom_density()
```

> But be careful about choosing the Gaussian distribution only when the plotted outcome variable looks Gaussian to you. Gawking at the raw data, to try to decide how to model them, is usually not a good idea. The data could be a mixture of different Gaussian distributions, for example, and in that case you won’t be able to detect the underlying normality just by eyeballing the outcome distribution. Furthermore, the empirical distribution needn’t be actually Gaussian in order to justify using a Gaussian likelihood.

Consider the model:

<center>$\begin{eqnarray} { h }_{ i } & \sim  & Normal(\mu ,\sigma ) & \text{<- likelihood} \\ \mu  & \sim  & Normal(178,20) & \text{<- } \mu \text{ prior} \\ \sigma  & \sim  & Cauchy(0,1) & \text{<- } \sigma \text{ prior} \end{eqnarray}$</center>

> The short model above is sometimes described as assuming that the values ${ h }_{ i }$ are independent and identically distributed (i.e. i.i.d., iid, or IID). “iid” indicates that each value ${ h }_{ i }$ has the same probability function, independent of the other $h$ values and using the same parameters. A moment’s reflection tells us that this is hardly ever true, in a physical sense. Whether measuring the same distance repeatedly or studying a population of heights, it is hard to argue that every measurement is independent of the others. The i.i.d. assumption doesn’t have to seem awkward, however, as long as you remember that probability is inside the golem, not outside in the world. The i.i.d. assumption is about how the golem represents its uncertainty. It is an epistemological assumption. It is not a physical assumption about the world, an ontological one, unless you insist that it is.

> Note that there’s no reason to expect a hard upper bound on $\sigma$.

```{r message=FALSE}
# Plot prior for mu
prior_mu <- ggplot(data = tibble(x = seq(from = 100, to = 250, by = .1)), 
                   aes(x = x, y = dnorm(x, mean = 178, sd = 20))) +
    geom_line() +
    xlab(expression(mu)) +
    ylab("density")

# Plot prior for sigma
prior_sigma <- ggplot(data = tibble(x = seq(from = -10, to = 60, by = 1)),
                      aes(x = x, y = dunif(x, min = 0, max = 50))) +
    geom_line() +
    scale_y_continuous(NULL, breaks = NULL) +
    xlab(expression(sigma))

# Plot heights by sampling from the prior
sample_mu <- rnorm(1e4, 178, 20)
sample_sigma <- runif(1e4, 0, 50)

heights <- tibble(x = rnorm(1e4, sample_mu, sample_sigma)) %>%
    ggplot(aes(x = x)) +
    geom_density() +
    scale_y_continuous(NULL, breaks = NULL) +
    xlab("h")

library(gridExtra)
grid.arrange(prior_mu, prior_sigma, heights, ncol=3)
```

### 4.3.5. Fitting the model with `brm`
```{r}
# Detach rethinking package
detach(package:rethinking, unload = T)

# Load brms
library(brms)

# Construct the model and set the priors
m4.1 <- brm(data = d2, family = gaussian(),
            height ~ 1,
            prior = c(set_prior("normal(178, 20)", class = "Intercept"),
                      set_prior("cauchy(0, 1)", class = "sigma")),
            chains = 4, iter = 2000, warmup = 1000, cores = 4)

# Plot the chains
plot(m4.1)
```

```{r}
# Summarise the model
summary(m4.1, prob = 0.89)
```

**Note**: `Est.Error` is equivalent to `StdDev`.

### 4.3.6. Sampling from a `brm` fit
```{r}
# Extract the iterations of the HMC chains and put them in a data frame
post <- posterior_samples(m4.1)

# Summarise the samples
t(apply(post[ , 1:2], 2, quantile, probs = c(.5, .055, .945)))
```

```{r}
# Using the tidyverse for summarising
post %>%
  select(-lp__) %>% 
  gather(parameter) %>%
  group_by(parameter) %>%
  summarise(mean = mean(value),
            SD   = sd(value),
            `5.5_percentile`  = quantile(value, probs = .055),
            `94.5_percentile` = quantile(value, probs = .945)) %>%
  mutate_if(is.numeric, round, digits = 2)
```

```{r}
# The rethinking package has already a function to summarise the samples
rethinking::precis(post[ , 1:2])
```

## 4.4. Adding a predictor

<center>$\begin{eqnarray} { h }_{ i } & \sim  & Normal({ \mu  }_{ i },\sigma ) & \text{<- likelihood } \\ { \mu  }_{ i } & = & \alpha +\beta { x }_{ i } & \text{<- linear model} \\ \alpha  & \sim  & Normal(178,100) & \text{<- }\alpha \text{ prior } \\ \beta  & \sim  & Normal(0,10) & \text{<- }\beta \text{ prior } \\ \sigma  & \sim  & Cauchy(0,1) & \text{<- }\sigma \text{ prior } \end{eqnarray}$</center>

> The linear model is asking two questions about the mean of the outcome:

> 1. What is the expected height, when $x_i = 0$? The parameter $\alpha$ answers this question.
2. What is the change in expected height, when $x_i$ changes by 1 unit? The parameter $\beta$ answers this question.

> The prior for $\beta$ places just as much probability below zero as it does above zero, and when $\beta = 0$, weight has no relationship to height. Such a prior will pull probability mass towards zero, leading to more conservative estimates than a perfectly flat prior will.

> **What’s the correct prior?** There is no more a uniquely correct prior than there is a uniquely correct likelihood. In choosing priors, there are simple guidelines to get you started. Priors encode states of information before seeing data. So priors allow us to explore the consequences of beginning with different information. In cases in which we have good prior information that discounts the plausibility of some parameter values, we can encode that information directly into priors. When we don’t have such information, we still usually know enough about the plausible range of values. And you can vary the priors and repeat the analysis in order to study how different states of initial information influence inference. Frequently, there are many reasonable choices for a prior, and all of them produce the same inference. And conventional Bayesian priors are conservative, relative to conventional non-Bayesian approaches.

### 4.4.2. Fitting the model
```{r message=FALSE}
m4.3 <- brm(height ~ 1 + weight,
            data = d2,
            family = gaussian(),
            prior = c(set_prior("normal(178,100)", class = "Intercept"),
                      set_prior("normal(0,10)", class = "b"),
                      set_prior("cauchy(0,1)", class = "sigma")),
            chains = 4, iter = 41000, warmup = 40000, cores = 4)

plot(m4.3)
```


## References
McElreath, R. (2016). *Statistical rethinking: A Bayesian course with examples in R and Stan.* Chapman & Hall/CRC Press.

Kurz, A. S. (2018, March 9). *brms, ggplot2 and tidyverse code, by chapter*. Retrieved from https://goo.gl/JbvNTj
