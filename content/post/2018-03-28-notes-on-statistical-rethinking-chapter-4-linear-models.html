---
title: Notes on Statistical Rethinking (Chapter 4 - Linear Models)
author: José Roberto Ayala Solares
date: '2018-03-28'
categories:
  - StatisticalRethinking
tags:
  - bayesian
  - notes
slug: notes-on-statistical-rethinking-chapter-4-linear-models
summary: Notes for Chapter 4 of [Statistical Rethinking](http://xcelab.net/rm/statistical-rethinking/)
---



<blockquote>
<p>Linear regression is a family of simple statistical golems that attempt to learn about the mean and variance of some measurement, using an additive combination of other measurements. Linear regression can usefully describe a very large variety of natural phenomena, and it is a descriptive model that corresponds to many different process models.</p>
</blockquote>
<div id="why-normal-distributions-are-normal" class="section level2">
<h2>4.1. Why normal distributions are normal</h2>
<div id="normal-by-addition" class="section level3">
<h3>4.1.1. Normal by addition</h3>
<blockquote>
<p>Any process that adds together random values from the same distribution converges to a normal.</p>
</blockquote>
<pre class="r"><code>library(tidyverse)

set.seed(1000)
pos &lt;- 
  replicate(100, runif(16, -1, 1)) %&gt;%        # Here is the simulation
  as_tibble() %&gt;%                             # For data manipulation, we will make this a tibble
  rbind(0, .) %&gt;%                             # Here we add a row of zeros above the simulation results
  mutate(step = 0:16) %&gt;%                     # This adds our step index
  gather(key, value, -step) %&gt;%               # Here we convert the data to the long format
  mutate(person = rep(1:100, each = 17)) %&gt;%  # This adds a person id index
  # The next two lines allows us to make culmulative sums within each person
  group_by(person) %&gt;%
  mutate(position = cumsum(value)) %&gt;%
  ungroup()  # Ungrouping allows for further data manipulation

pos %&gt;%
  filter(step == 16) %&gt;%
  ggplot(aes(x = position)) +
  geom_density(color = &quot;dodgerblue1&quot;) + #geom_line(stat = &quot;density&quot;, color = &quot;dodgerblue1&quot;) +
  coord_cartesian(xlim = -6:6) +
  labs(title = &quot;16 steps&quot;)</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
</div>
<div id="normal-by-multiplication" class="section level3">
<h3>4.1.2. Normal by multiplication</h3>
<blockquote>
<p>Small effects that multiply together are approximately additive, and so they also tend to stabilize on Gaussian distributions.</p>
</blockquote>
<pre class="r"><code>set.seed(.1)
growth &lt;- 
  replicate(10000, prod(1 + runif(12, 0, 0.1))) %&gt;%
  as_tibble()

ggplot(data = growth, aes(x = value)) +
  geom_density(color = &quot;dodgerblue1&quot;)</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="normal-by-log-multiplication" class="section level3">
<h3>4.1.3. Normal by log-multiplication</h3>
<blockquote>
<p>Large deviates that are multiplied together do not produce Gaussian distributions, but they do tend to produce Gaussian distributions on the log scale.</p>
</blockquote>
<pre class="r"><code>set.seed(12)
replicate(10000, log(prod(1 + runif(12,0,0.5)))) %&gt;%
    as_tibble() %&gt;%
    ggplot(aes(x = value)) +
    geom_density(color = &quot;dodgerblue1&quot;)</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="using-gaussian-distributions" class="section level3">
<h3>4.1.4. Using Gaussian distributions</h3>
<div id="ontological-justification" class="section level4">
<h4>4.1.4.1. Ontological justification</h4>
<blockquote>
<p>The world is full of Gaussian distributions, approximately. As a mathematical idealization, we’re never going to experience a perfect Gaussian distribution. But it is a widespread pattern, appearing again and again at different scales and in different domains. Measurement errors, variations in growth, and the velocities of molecules all tend towards Gaussian distributions. These processes do this because at their heart, these processes add together fluctuations. And repeatedly adding finite fluctuations results in a distribution of sums that have shed all information about the underlying process, aside from mean and spread. One consequence of this is that statistical models based on Gaussian distributions cannot reliably identify micro-process. But it also means that these models can do useful work, even when they cannot identify process.</p>
</blockquote>
</div>
<div id="epistemological-justification" class="section level4">
<h4>4.1.4.2 Epistemological justification</h4>
<blockquote>
<p>The Gaussian distribution is the most natural expression of our state of ignorance, because if all we are willing to assume is that a measure has finite variance, the Gaussian distribution is the shape that can be realized in the largest number of ways and does not introduce any new assumptions. <strong>It is the least surprising and least informative assumption to make.</strong></p>
</blockquote>
</div>
</div>
</div>
<div id="a-gaussian-model-of-height" class="section level2">
<h2>4.3. A Gaussian model of height</h2>
<div id="the-data" class="section level3">
<h3>4.3.1. The data</h3>
<pre class="r"><code># Load data from the rethinking package
library(rethinking)
data(Howell1)
d &lt;- Howell1

# Have a look at the data
d %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 544
## Variables: 4
## $ height &lt;dbl&gt; 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8...
## $ weight &lt;dbl&gt; 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99...
## $ age    &lt;dbl&gt; 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 5...
## $ male   &lt;int&gt; 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1...</code></pre>
<pre class="r"><code># Filter heights of adults in the sample. The reason to filter out non-adults is that height is strongly correlated with age, before adulthood.
d2 &lt;- d %&gt;% filter(age &gt;= 18) %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 352
## Variables: 4
## $ height &lt;dbl&gt; 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8...
## $ weight &lt;dbl&gt; 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99...
## $ age    &lt;dbl&gt; 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 5...
## $ male   &lt;int&gt; 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1...</code></pre>
</div>
<div id="the-model" class="section level3">
<h3>4.3.2. The model</h3>
<pre class="r"><code>d2 %&gt;%
    ggplot(aes(x = height)) + 
    geom_density()</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<blockquote>
<p>But be careful about choosing the Gaussian distribution only when the plotted outcome variable looks Gaussian to you. Gawking at the raw data, to try to decide how to model them, is usually not a good idea. The data could be a mixture of different Gaussian distributions, for example, and in that case you won’t be able to detect the underlying normality just by eyeballing the outcome distribution. Furthermore, the empirical distribution needn’t be actually Gaussian in order to justify using a Gaussian likelihood.</p>
</blockquote>
<p>Consider the model:</p>
<p><span class="math display">\[\begin{eqnarray} { h }_{ i } &amp; \sim  &amp; Normal(\mu ,\sigma ) &amp; \text{&lt;- likelihood} \\ \mu  &amp; \sim  &amp; Normal(178,20) &amp; \text{&lt;- } \mu \text{ prior} \\ \sigma  &amp; \sim  &amp; Cauchy(0,1) &amp; \text{&lt;- } \sigma \text{ prior} \end{eqnarray}\]</span></p>
<blockquote>
<p>The short model above is sometimes described as assuming that the values <span class="math inline">\({ h }_{ i }\)</span> are independent and identically distributed (i.e. i.i.d., iid, or IID). “iid” indicates that each value <span class="math inline">\({ h }_{ i }\)</span> has the same probability function, independent of the other <span class="math inline">\(h\)</span> values and using the same parameters. A moment’s reflection tells us that this is hardly ever true, in a physical sense. Whether measuring the same distance repeatedly or studying a population of heights, it is hard to argue that every measurement is independent of the others. The i.i.d. assumption doesn’t have to seem awkward, however, as long as you remember that probability is inside the golem, not outside in the world. The i.i.d. assumption is about how the golem represents its uncertainty. It is an epistemological assumption. It is not a physical assumption about the world, an ontological one, unless you insist that it is.</p>
</blockquote>
<p>{{% alert note %}} There’s no reason to expect a hard upper bound on <span class="math inline">\(\sigma\)</span>. {{% /alert %}}</p>
<pre class="r"><code># Plot prior for mu
prior_mu &lt;- ggplot(data = tibble(x = seq(from = 100, to = 250, by = .1)), 
                   aes(x = x, y = dnorm(x, mean = 178, sd = 20))) +
    geom_line() +
    xlab(expression(mu)) +
    ylab(&quot;density&quot;)

# Plot prior for sigma
prior_sigma &lt;- ggplot(data = tibble(x = seq(from = -10, to = 60, by = 1)),
                      aes(x = x, y = dunif(x, min = 0, max = 50))) +
    geom_line() +
    scale_y_continuous(NULL, breaks = NULL) +
    xlab(expression(sigma))

# Plot heights by sampling from the prior
sample_mu &lt;- rnorm(1e4, 178, 20)
sample_sigma &lt;- runif(1e4, 0, 50)

heights &lt;- tibble(x = rnorm(1e4, sample_mu, sample_sigma)) %&gt;%
    ggplot(aes(x = x)) +
    geom_density() +
    scale_y_continuous(NULL, breaks = NULL) +
    xlab(&quot;h&quot;)

library(gridExtra)
grid.arrange(prior_mu, prior_sigma, heights, ncol=3)</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="fitting-the-model-with-brm" class="section level3">
<h3>4.3.5. Fitting the model with <code>brm</code></h3>
<pre class="r"><code># Detach rethinking package
detach(package:rethinking, unload = T)

# Load brms
library(brms)</code></pre>
<pre><code>## Loading required package: Rcpp</code></pre>
<pre><code>## Warning: package &#39;Rcpp&#39; was built under R version 3.4.4</code></pre>
<pre><code>## Loading &#39;brms&#39; package (version 2.1.0). Useful instructions
## can be found by typing help(&#39;brms&#39;). A more detailed introduction
## to the package is available through vignette(&#39;brms_overview&#39;).
## Run theme_set(theme_default()) to use the default bayesplot theme.</code></pre>
<pre class="r"><code># Construct the model and set the priors
m4.1 &lt;- brm(data = d2, family = gaussian(),
            height ~ 1,
            prior = c(set_prior(&quot;normal(178, 20)&quot;, class = &quot;Intercept&quot;),
                      set_prior(&quot;cauchy(0, 1)&quot;, class = &quot;sigma&quot;)),
            chains = 4, iter = 2000, warmup = 1000, cores = 4)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>
<pre class="r"><code># Plot the chains
plot(m4.1)</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code># Summarise the model
summary(m4.1, prob = 0.89)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: height ~ 1 
##    Data: d2 (Number of observations: 352) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; 
##          total post-warmup samples = 4000
##     ICs: LOO = NA; WAIC = NA; R2 = NA
##  
## Population-Level Effects: 
##           Estimate Est.Error l-89% CI u-89% CI Eff.Sample Rhat
## Intercept   154.61      0.40   153.98   155.26       3454 1.00
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-89% CI u-89% CI Eff.Sample Rhat
## sigma     7.75      0.29     7.31     8.22       3170 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>{{% alert note %}} <code>Est.Error</code> is equivalent to <code>StdDev</code>. {{% /alert %}}</p>
</div>
<div id="sampling-from-a-brm-fit" class="section level3">
<h3>4.3.6. Sampling from a <code>brm</code> fit</h3>
<pre class="r"><code># Extract the iterations of the HMC chains and put them in a data frame
post &lt;- posterior_samples(m4.1)

# Summarise the samples
t(apply(post[ , 1:2], 2, quantile, probs = c(.5, .055, .945)))</code></pre>
<pre><code>##                    50%       5.5%      94.5%
## b_Intercept 154.609590 153.979622 155.261765
## sigma         7.736716   7.313118   8.217413</code></pre>
<pre class="r"><code># Using the tidyverse for summarising
post %&gt;%
  select(-lp__) %&gt;% 
  gather(parameter) %&gt;%
  group_by(parameter) %&gt;%
  summarise(mean = mean(value),
            SD   = sd(value),
            `5.5_percentile`  = quantile(value, probs = .055),
            `94.5_percentile` = quantile(value, probs = .945)) %&gt;%
  mutate_if(is.numeric, round, digits = 2)</code></pre>
<pre><code>## # A tibble: 2 x 5
##   parameter     mean    SD `5.5_percentile` `94.5_percentile`
##   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;
## 1 b_Intercept 155.   0.400           154.              155.  
## 2 sigma         7.75 0.290             7.31              8.22</code></pre>
<pre class="r"><code># The rethinking package has already a function to summarise the samples
rethinking::precis(post[ , 1:2])</code></pre>
<pre><code>##               Mean StdDev  |0.89  0.89|
## b_Intercept 154.61   0.40 153.98 155.25
## sigma         7.75   0.29   7.30   8.20</code></pre>
</div>
</div>
<div id="adding-a-predictor" class="section level2">
<h2>4.4. Adding a predictor</h2>
<p><span class="math display">\[\begin{eqnarray} { h }_{ i } &amp; \sim  &amp; Normal({ \mu  }_{ i },\sigma ) &amp; \text{&lt;- likelihood } \\ { \mu  }_{ i } &amp; = &amp; \alpha +\beta { x }_{ i } &amp; \text{&lt;- linear model} \\ \alpha  &amp; \sim  &amp; Normal(178,100) &amp; \text{&lt;- }\alpha \text{ prior } \\ \beta  &amp; \sim  &amp; Normal(0,10) &amp; \text{&lt;- }\beta \text{ prior } \\ \sigma  &amp; \sim  &amp; Cauchy(0,1) &amp; \text{&lt;- }\sigma \text{ prior } \end{eqnarray}\]</span></p>
<blockquote>
<p>The linear model is asking two questions about the mean of the outcome:</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>What is the expected height, when <span class="math inline">\(x_i = 0\)</span>? The parameter <span class="math inline">\(\alpha\)</span> answers this question. Parameters like <span class="math inline">\(\alpha\)</span> are “intercepts” that tell us the value of <span class="math inline">\(\mu\)</span> when all of the predictor variables have value zero. As a consequence, the value of the intercept is frequently uninterpretable without also studying any <span class="math inline">\(\beta\)</span> parameters.</li>
<li>What is the change in expected height, when <span class="math inline">\(x_i\)</span> changes by 1 unit? The parameter <span class="math inline">\(\beta\)</span> answers this question.</li>
</ol>
</blockquote>
<blockquote>
<p>The prior for <span class="math inline">\(\beta\)</span> places just as much probability below zero as it does above zero, and when <span class="math inline">\(\beta = 0\)</span>, weight has no relationship to height. Such a prior will pull probability mass towards zero, leading to more conservative estimates than a perfectly flat prior will.</p>
</blockquote>
<blockquote>
<p><strong>What’s the correct prior?</strong> There is no more a uniquely correct prior than there is a uniquely correct likelihood. In choosing priors, there are simple guidelines to get you started. Priors encode states of information before seeing data. So priors allow us to explore the consequences of beginning with different information. In cases in which we have good prior information that discounts the plausibility of some parameter values, we can encode that information directly into priors. When we don’t have such information, we still usually know enough about the plausible range of values. And you can vary the priors and repeat the analysis in order to study how different states of initial information influence inference. Frequently, there are many reasonable choices for a prior, and all of them produce the same inference. And conventional Bayesian priors are conservative, relative to conventional non-Bayesian approaches.</p>
</blockquote>
<div id="fitting-the-model" class="section level3">
<h3>4.4.2. Fitting the model</h3>
<pre class="r"><code>m4.3 &lt;- brm(height ~ 1 + weight,
            data = d2,
            family = gaussian(),
            prior = c(set_prior(&quot;normal(178,100)&quot;, class = &quot;Intercept&quot;),
                      set_prior(&quot;normal(0,10)&quot;, class = &quot;b&quot;),
                      set_prior(&quot;cauchy(0,1)&quot;, class = &quot;sigma&quot;)),
            chains = 4, iter = 41000, warmup = 40000, cores = 4)

plot(m4.3)</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
<div id="interpreting-the-model-fit" class="section level3">
<h3>4.4.3. Interpreting the model fit</h3>
<div id="tables-of-estimates" class="section level4">
<h4>4.4.3.1. Tables of estimates</h4>
<blockquote>
<p>Posterior probabilities of parameter values describe the relative compatibility of different states of the world with the data, according to the model.</p>
</blockquote>
<pre class="r"><code># Check the correlations among parameters
posterior_samples(m4.3) %&gt;%
    select(-lp__) %&gt;%
    cor() %&gt;%
    round(digits = 2)</code></pre>
<pre><code>##             b_Intercept b_weight sigma
## b_Intercept        1.00    -0.99 -0.01
## b_weight          -0.99     1.00  0.02
## sigma             -0.01     0.02  1.00</code></pre>
<pre class="r"><code>pairs(m4.3)</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>With centering, we can reduce the correlations among the parameters. Furthermore, by centering the predictor, the estimate for the intercept <span class="math inline">\(\alpha\)</span> corresponds to the expected value of the outcome, when the predictor is at its average value.</p>
<pre class="r"><code>d3 &lt;- d2 %&gt;%
    mutate(weight.c = weight - mean(weight))

m4.4 &lt;- brm(height ~ 1 + weight.c,
            data = d3,
            family = gaussian(), # default
            prior = c(set_prior(&quot;normal(178,100)&quot;, class = &quot;Intercept&quot;),
                      set_prior(&quot;normal(0,10)&quot;, class = &quot;b&quot;),
                      set_prior(&quot;cauchy(0,1)&quot;, class = &quot;sigma&quot;)),
            chains = 4, iter = 41000, warmup = 40000, cores = 4)

plot(m4.4)</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>posterior_samples(m4.4) %&gt;%
  select(-lp__) %&gt;% 
  gather(parameter) %&gt;%
  group_by(parameter) %&gt;%
  summarise(mean = mean(value),
            SD   = sd(value),
            `5.5_percentile`  = quantile(value, probs = .055),
            `94.5_percentile` = quantile(value, probs = .945)) %&gt;%
  mutate_if(is.numeric, round, digits = 2)</code></pre>
<pre><code>## # A tibble: 3 x 5
##   parameter      mean     SD `5.5_percentile` `94.5_percentile`
##   &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;
## 1 b_Intercept 155.    0.270           154.              155.   
## 2 b_weight.c    0.900 0.0400            0.840             0.970
## 3 sigma         5.09  0.190             4.79              5.41</code></pre>
<pre class="r"><code>posterior_samples(m4.4) %&gt;%
    select(-lp__) %&gt;%
    cor() %&gt;%
    round(digits = 2)</code></pre>
<pre><code>##             b_Intercept b_weight.c sigma
## b_Intercept        1.00      -0.01  0.00
## b_weight.c        -0.01       1.00 -0.01
## sigma              0.00      -0.01  1.00</code></pre>
<pre class="r"><code>pairs(m4.4)</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
<div id="plotting-posterior-inference-against-the-data" class="section level4">
<h4>4.4.3.2. Plotting posterior inference against the data</h4>
<pre class="r"><code>d2 %&gt;%
    ggplot(aes(x = weight, y = height)) +
    theme_bw() +
    geom_abline(intercept = fixef(m4.3)[1],
                slope = fixef(m4.3)[2]) +
  geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) +
  theme(panel.grid = element_blank())</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
<div id="plotting-regression-intervals-and-contours" class="section level4">
<h4>4.4.3.4. Plotting regression intervals and contours</h4>
<p>{{% alert note %}} <code>brms::fitted()</code> is the anologue to <code>rethinking::link()</code>. {{% /alert %}}</p>
<pre class="r"><code># When you specify summary = F, fitted() returns a matrix of values with as many rows as there were post-warmup iterations across your HMC chains and as many columns as there were cases in your analysis.
mu &lt;- fitted(m4.3, summary = F)

# Plot regression line and its intervals
weight_seq &lt;- tibble(weight = seq(from = 25, to = 70, by = 1))
muSummary &lt;- fitted(m4.3,
                    newdata = weight_seq,
                    probs = c(0.055, 0.945)) %&gt;%
    as_tibble() %&gt;%
    bind_cols(weight_seq) %&gt;%
    walk(head)

d2 %&gt;%
    ggplot(aes(x = weight, y = height)) +
    geom_ribbon(data = muSummary, 
                aes(y = Estimate, ymin = `5.5%ile`, ymax = `94.5%ile`),
                fill = &quot;grey70&quot;) +
    geom_line(data = muSummary,
              aes(y = Estimate),
              color = &quot;red&quot;) +
    geom_point(color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 2/3) +
    theme(text = element_text(family = &quot;Times&quot;),
          panel.grid = element_blank())</code></pre>
<pre><code>## Warning: Ignoring unknown aesthetics: y</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
<div id="prediction-intervals" class="section level4">
<h4>4.4.3.5. Prediction intervals</h4>
<p>{{% alert note %}} <code>brms::predict()</code> is the anologue to <code>rethinking::sim()</code>. {{% /alert %}}</p>
<pre class="r"><code># The summary information in our data frame is for simulated heights, not distributions of plausible average height, $\mu$
pred_height &lt;- predict(m4.3,
                       newdata = weight_seq,
                       probs = c(0.055, 0.945)) %&gt;%
    as_tibble() %&gt;%
    bind_cols(weight_seq)

d2 %&gt;%
    ggplot(aes(x = weight, y = height)) +
    geom_ribbon(data = pred_height, 
                aes(y = Estimate, ymin = `5.5%ile`, ymax = `94.5%ile`),
                fill = &quot;grey83&quot;) +
    geom_ribbon(data = muSummary, 
                aes(y = Estimate, ymin = `5.5%ile`, ymax = `94.5%ile`),
                fill = &quot;grey70&quot;) +
    geom_line(data = muSummary,
              aes(y = Estimate),
              color = &quot;red&quot;) +
    geom_point(color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 2/3) +
    theme(text = element_text(family = &quot;Times&quot;),
          panel.grid = element_blank())</code></pre>
<pre><code>## Warning: Ignoring unknown aesthetics: y

## Warning: Ignoring unknown aesthetics: y</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>{{% alert note %}} The outline for the wide shaded interval is a little jagged. This is the simulation variance in the tails of the sampled Gaussian values. Increase the number of samples to reduce it. With extreme percentiles, it can be very hard to get out all of the jaggedness. {{% /alert %}}</p>
<blockquote>
<p><strong>Two kinds of uncertainty</strong>. In the procedure above, we encountered both uncertainty in parameter values and uncertainty in a sampling process. These are distinct concepts, even though they are processed much the same way and end up blended together in the <strong>posterior predictive simulation</strong> (check <a href="https://jroberayalas.netlify.com/post/notes-on-statistical-rethinking-chapter-3-sampling-the-imaginary/">Chapter 3</a>). The posterior distribution is a ranking of the relative plausibilities of every possible combination of parameter values. The distribution of simulated outcomes is instead a distribution that includes sampling variation from some process that generates Gaussian random variables. This sampling variation is still a model assumption. It’s no more or less objective than the posterior distribution. Both kinds of uncertainty matter, at least sometimes. But it’s important to keep them straight, because they depend upon different model assumptions. Furthermore, it’s possible to view the Gaussian likelihood as a purely epistemological assumption (a device for estimating the mean and variance of a variable), rather than an ontological assumption about what future data will look like. In that case, it may not make complete sense to simulate outcomes.</p>
</blockquote>
</div>
</div>
</div>
<div id="polynomial-regression" class="section level2">
<h2>4.5. Polynomial regression</h2>
<p>In general, it’s not good to use polynomial regression because polynomials are very hard to interpret. Better would be to have a more mechanistic model of the data, one that builds the non-linear relationship up from a principled beginning.</p>
<blockquote>
<p>Standarization of a predictor means to first center the variable and then divide it by its standard deviation. This leaves the mean at zero but also rescales the range of the data. This is helpful for two reasons:</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>Interpretation might be easier. For a standardized variable, a change of one unit is equivalent to a change of one standard deviation. In many contexts, this is more interesting and more revealing than a one unit change on the natural scale. And once you start making regressions with more than one kind of predictor variable, standardizing all of them makes it easier to compare their relative influence on the outcome, using only estimates. On the other hand, you might want to interpret the data on the natural scale. So standardization can make interpretation harder, not easier.</li>
<li>More important though are the advantages for fitting the model to the data. When predictor variables have very large values in them, there are sometimes numerical glitches. Even well-known statistical software can suffer from these glitches, leading to mistaken estimates. These problems are very common for polynomial regression, because the square or cube of a large number can be truly massive. Standardizing largely resolves this issue.</li>
</ol>
</blockquote>
<pre class="r"><code>d &lt;-
  d %&gt;%
  mutate(weight.s = (weight - mean(weight))/sd(weight),
         weight.s2 = weight.s^2)</code></pre>
<p>Consider the following parabolic model:</p>
<p><span class="math display">\[\begin{eqnarray} { h }_{ i } &amp; \sim  &amp; Normal({ \mu  }_{ i },\sigma ) &amp; \text{&lt;- likelihood } \\ { \mu  }_{ i } &amp; = &amp; \alpha +{ \beta  }_{ 1 }{ x }_{ i }+{ \beta  }_{ 2 }{ x }_{ i }^{ 2 } &amp; \text{&lt;- linear model } \\ \alpha  &amp; \sim  &amp; Normal(178,100) &amp; \text{&lt;- }\alpha \text{ prior } \\ { \beta  }_{ 1 } &amp; \sim  &amp; Normal(0,10) &amp; \text{&lt;- }{ \beta  }_{ 1 }\text{ prior } \\ { \beta  }_{ 2 } &amp; \sim  &amp; Normal(0,10) &amp; \text{&lt;- }{ \beta  }_{ 2 } \text{ prior} \\ \sigma  &amp; \sim  &amp; Cauchy(0,1) &amp; \text{&lt;- }\sigma \text{ prior } \end{eqnarray}\]</span></p>
<pre class="r"><code>d %&gt;%
    ggplot(aes(x = weight.s, y = height)) +
    geom_point(color = &quot;navyblue&quot;, shape = 1, size = 1.5)</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<pre class="r"><code>m4.5 &lt;- brm(height ~ 1 + weight.s + I(weight.s^2),
            data = d,
            prior = c(set_prior(&quot;normal(178,100)&quot;, class = &quot;Intercept&quot;),
                      set_prior(&quot;normal(0,10)&quot;, class = &quot;b&quot;),
                      set_prior(&quot;cauchy(0,1)&quot;, class = &quot;sigma&quot;)),
            chains = 4, iter = 2000, warmup = 1000, cores = 4)

plot(m4.5)</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code>posterior_samples(m4.5) %&gt;%
  select(-lp__) %&gt;% 
  gather(parameter) %&gt;%
  group_by(parameter) %&gt;%
  summarise(mean = mean(value),
            SD   = sd(value),
            `5.5_percentile`  = quantile(value, probs = .055),
            `94.5_percentile` = quantile(value, probs = .945)) %&gt;%
  mutate_if(is.numeric, round, digits = 2)</code></pre>
<pre><code>## # A tibble: 4 x 5
##   parameter        mean    SD `5.5_percentile` `94.5_percentile`
##   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;
## 1 b_Intercept    147.   0.360           146.              147.  
## 2 b_Iweight.sE2   -8.42 0.270            -8.85             -7.99
## 3 b_weight.s      21.4  0.280            20.9              21.8 
## 4 sigma            5.77 0.180             5.49              6.06</code></pre>
<pre class="r"><code>posterior_samples(m4.5) %&gt;%
    select(-lp__) %&gt;%
    cor() %&gt;%
    round(digits = 2)</code></pre>
<pre><code>##               b_Intercept b_weight.s b_Iweight.sE2 sigma
## b_Intercept          1.00      -0.36         -0.74  0.02
## b_weight.s          -0.36       1.00          0.49 -0.04
## b_Iweight.sE2       -0.74       0.49          1.00 -0.03
## sigma                0.02      -0.04         -0.03  1.00</code></pre>
<pre class="r"><code>pairs(m4.5)</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<pre class="r"><code>weight_seq &lt;- data.frame(weight.s = seq(from = -2.2, to = 2, length.out = 30))

muSummary &lt;- fitted(m4.5,
                    newdata = weight_seq,
                    probs = c(0.055, 0.945)) %&gt;%
    as_tibble() %&gt;%
    bind_cols(weight_seq)

pred_height &lt;- predict(m4.5,
                       newdata = weight_seq,
                       probs = c(0.055, 0.945)) %&gt;%
    as_tibble() %&gt;%
    bind_cols(weight_seq)

d %&gt;%
    ggplot(aes(x = weight.s, y = height)) +
    geom_ribbon(data = pred_height, 
                aes(y = Estimate, ymin = `5.5%ile`, ymax = `94.5%ile`),
                fill = &quot;grey83&quot;) +
    geom_ribbon(data = muSummary, 
                aes(y = Estimate, ymin = `5.5%ile`, ymax = `94.5%ile`),
                fill = &quot;grey70&quot;) +
    geom_line(data = muSummary,
              aes(y = Estimate),
              color = &quot;red&quot;) +
    geom_point(color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 2/3) +
    theme(text = element_text(family = &quot;Times&quot;),
          panel.grid = element_blank())</code></pre>
<pre><code>## Warning: Ignoring unknown aesthetics: y

## Warning: Ignoring unknown aesthetics: y</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre class="r"><code># Correct the x scale
at &lt;- c(-2, -1, 0, 1, 2)

d %&gt;%
    ggplot(aes(x = weight.s, y = height)) +
    geom_ribbon(data = pred_height, 
                aes(y = Estimate, ymin = `5.5%ile`, ymax = `94.5%ile`),
                fill = &quot;grey83&quot;) +
    geom_ribbon(data = muSummary, 
                aes(y = Estimate, ymin = `5.5%ile`, ymax = `94.5%ile`),
                fill = &quot;grey70&quot;) +
    geom_line(data = muSummary,
              aes(y = Estimate),
              color = &quot;red&quot;) +
    geom_point(color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 2/3) +
    theme(text = element_text(family = &quot;Times&quot;),
          panel.grid = element_blank()) +
    # Here it is!
    scale_x_continuous(breaks = at,
                       labels = round(at*sd(d$weight) + mean(d$weight), 1))</code></pre>
<pre><code>## Warning: Ignoring unknown aesthetics: y

## Warning: Ignoring unknown aesthetics: y</code></pre>
<p><img src="/post/2018-03-28-notes-on-statistical-rethinking-chapter-4-linear-models_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>McElreath, R. (2016). <em>Statistical rethinking: A Bayesian course with examples in R and Stan.</em> Chapman &amp; Hall/CRC Press.</p>
<p>Kurz, A. S. (2018, March 9). <em>brms, ggplot2 and tidyverse code, by chapter</em>. Retrieved from <a href="https://goo.gl/JbvNTj" class="uri">https://goo.gl/JbvNTj</a></p>
</div>
